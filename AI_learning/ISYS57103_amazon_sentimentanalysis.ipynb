{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "import datasets\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "datasets.logging.set_verbosity_error()\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset \n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate \n",
    "\n",
    "#BERT tokenizer (wordpiece), splitting sequence tokens available in the its libary\n",
    "#   if it sess a word it doesnt know \"VRAM\" it will split it into \"V\" \"Ra\" \"M\", a double-hash prefix \"##RA\" added\n",
    "\n",
    "#(hyperparameters are external configurations, parameters are learned during training (weight), batchsize, learning rate, )\n",
    "\n",
    "\n",
    "# --- load  Pre-trained model \n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "#What does autotokenizer do? \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "#--- load dataset\n",
    "# split = \"train\" returns only the training side \n",
    "# could do {\"train\": 0.8, \"test\": 0.2} \n",
    "raw_datasets = load_dataset(\"hugginglearners/amazon-reviews-sentiment-analysis\", split = \"train\")\n",
    "\n",
    "\n",
    "#--- Data Cleaning\n",
    "#check nulls \n",
    "sum_nulls = sum(r is None for r in raw_datasets[\"reviewText\"])\n",
    "#print(sum_nulls)\n",
    "\n",
    "#check how types are not \"str\" \n",
    "sum_notstrings = sum(not isinstance(r,str) for r in raw_datasets['reviewText'])\n",
    "#print(sum_notstrings)\n",
    "\n",
    "#bad indices \n",
    "bad_indices = [i for i, r in enumerate(raw_datasets[\"reviewText\"]) if not isinstance(r, str) or r is None]\n",
    "#print(\"First few bad rows:\", bad_indices[:10])\n",
    "\n",
    "#print a few bad examples to understand\n",
    "for i in bad_indices[:3]:\n",
    "    print(i, raw_datasets[i])\n",
    "\n",
    "\n",
    "#print(raw_datasets.features)\n",
    "\n",
    "#--- Tokenization example --- \n",
    "#Tokenization converts all text into tokens (input_ids)\n",
    "#Padding ensures all sentences have the same len \n",
    "#Truncation makes sure any length beyond \"max_length\" is cut off. \n",
    "\n",
    "sample_texts = raw_datasets[\"reviewText\"][:10]  # list of 10 samples\n",
    "variable = tokenizer(\n",
    "                sample_texts,\n",
    "                truncation=True,\n",
    "                padding= True            \n",
    ")\n",
    "\n",
    "#print(variable[\"input_ids\"]) -- inputs id are numbers from words [I am cool] = [101,23,34]\n",
    "#print(variable[\"attention_mask\"]) -- indicates which padding should the model attend to, 1 yes, 0 no [1,1,1,0,0,0]\n",
    "\n",
    "\n",
    "\n",
    "# --- Tokenize whole dataset ---\n",
    "#to efficiently preprocess data, we use .map()\n",
    "def tokenize_function(batch: Dict[str, List[Any]]):\n",
    "    texts = [str(t) if t is not None else \"\" for t in batch[\"reviewText\"]]\n",
    "    #padding will add len to the short to have the same length as the longest one \n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "#batched = True, process batches of rows for speed\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function,batched=True)\n",
    "#print(tokenized_datasets)\n",
    "\n",
    "# --- Dynamic Padding ---\n",
    "#performs dynamic padding per batch, not dataset-wide padding. if first batch [46,34,56] it will choose 56, if second batch [23,45,67], it will use 67\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#Example\n",
    "#test to see dynamic padding, first we see that the output as various lengths \n",
    "samples = raw_datasets['reviewText'][:8]\n",
    "encoded = tokenizer(samples, padding=False, truncation=False)\n",
    "lengths = [len(x) for x in encoded[\"input_ids\"]]   # counts tokens\n",
    "print(\"Token lengths before padding:\",lengths)\n",
    "\n",
    "#this shows that the padded length is up to 98 since 98 is the largest within this 8 rows \n",
    "# Properly show how data_collator pads a batch\n",
    "batch = data_collator([{\"input_ids\": i, \"attention_mask\": [1]*len(i)} for i in encoded[\"input_ids\"]])\n",
    "print(\"Batch tensor shapes after padding:\", {k: v.shape for k, v in batch.items()})\n",
    "\n",
    "\n",
    "\n",
    "# --- training configuration ---\n",
    "# TrainingArguments defines hyperparameters for the Trainer class.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test-trainer\",          # where to save checkpoints\n",
    "    num_train_epochs=5,                 # training for 5 epochs\n",
    "    learning_rate=2e-5,                 # standard for BERT fine-tuning\n",
    "    per_device_train_batch_size=8,      # batch size per GPU/CPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=100,                  # logs every 100 steps\n",
    "    fp16=torch.cuda.is_available(),     # use fp16 only if GPU supports it\n",
    "    evaluation_strategy=\"steps\",        # use 'steps' for compatibility\n",
    "    eval_steps=500,                     # evaluate every 500 steps\n",
    "    save_strategy=\"steps\",              # also save checkpoints every few steps\n",
    "    save_steps=500,                     # number of steps to save\n",
    "    logging_dir=\"./logs\",               # folder for TensorBoard logs\n",
    "    load_best_model_at_end=True         # optional: automatically load best model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 8️⃣ Load Model ---\n",
    "# Load pretrained DistilBERT model for binary classification (positive/negative). what does num_labels do? \n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "#--- Split data into train / test\n",
    "train = tokenized_datasets.train_test_split(test_size = 0.2)\n",
    "#print(train)\n",
    "\n",
    "#logits return the raw outputs from the model before softmax\n",
    "#labels return the target labels (1,0)\n",
    "def compute_metrics(eval_preds):\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    "    load_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    }\n",
    "\n",
    "# --- Define a trainer class ---\n",
    "# Trainer wraps model, data, and config, similar to an sklearn pipeline\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args = training_args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #how do we split the amazon dataset? \n",
    "    train_dataset = train['train'],\n",
    "    eval_dataset = train['test'], \n",
    "    tokenizer = tokenizer # we are telling the trainer which tokenizer to use \n",
    ")\n",
    "\n",
    "trainer.train() #run on GPU \n",
    "\n",
    "# --- Evaluate Model ---\n",
    "predictions = trainer.predict(tokenized_datasets[\"train\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check nulls for raw_datasets\n",
    "sum_nulls = sum(r is None for r in raw_datasets[\"reviewText\"])\n",
    "print(sum_nulls)\n",
    "\n",
    "#check not strings \n",
    "sum_notstrings = sum(not isinstance(r,str) for r in raw_datasets['reviewText'])\n",
    "print(sum_notstrings)\n",
    "\n",
    "#bad indices \n",
    "bad_indices = [i for i, r in enumerate(raw_datasets[\"reviewText\"]) if not isinstance(r, str) or r is None]\n",
    "print(\"First few bad rows:\", bad_indices[:10])\n",
    "\n",
    "for i in bad_indices[:3]:\n",
    "    print(i, raw_datasets[i])\n",
    "\n",
    "\n",
    "    #convert IDs back to tokens \n",
    "for k, v in enumerate(variable['input_ids']):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(v)\n",
    "    print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986017b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
